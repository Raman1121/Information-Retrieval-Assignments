{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import en_core_web_sm\n",
    "from pattern.en import suggest\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "nlp = spacy.load('en_core_web_sm', parse=True, tag=True, entity=True)\n",
    "tokenizer = ToktokTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = open(\"A Boy's Will - Robert Frost/1.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#txt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_new = init_process(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_process(txt):\n",
    "    no_new = re.sub('\\n', ' ', txt)\n",
    "    no_spl = re.sub('Ã±', ' ', no_new)\n",
    "    first_parse = re.sub(r'[^\\w]', ' ', no_spl)\n",
    "    return first_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling(w):\n",
    "    word_wlf = reduce_lengthening(w)\n",
    "    correct_word = suggest(word_wlf)\n",
    "    return correct_word[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spelling_correction(words):\n",
    "    correct = [correct_spelling(w) for w in words]\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    #filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(file_name):\n",
    "    file = open(file_name, \"r\")\n",
    "    read_txt = file.read()\n",
    "    list_new = init_process(read_txt)\n",
    "    stemmed = stemming(list_new)\n",
    "    lemma = lemmatize_text(stemmed)\n",
    "    new_words = remove_stopwords(lemma)\n",
    "    final = spelling_correction(new_words)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_text():\n",
    "    for i in range(1,51):\n",
    "        new_text_file = str(i)+\".txt\"\n",
    "        file_name = \"poems/\"+new_text_file\n",
    "        refined_list = normalise(file_name)\n",
    "        refined_text = ' '.join(refined_list)\n",
    "        text_file = open((\"refined/\"+new_text_file), \"w\")\n",
    "        text_file.write(refined_text)\n",
    "        text_file.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refine_text() # Dont refine everytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary = {}\n",
    "# for i in range(1,51):\n",
    "#     file_name = (\"refined/\"+str(i)+\".txt\")\n",
    "#     text = open(file_name, \"r\").read()\n",
    "#     tokens = tokenizer.tokenize(text)\n",
    "#     for t in tokens:\n",
    "#         if t in dictionary:\n",
    "#             dictionary.get(t).append(i)\n",
    "#             dictionary[t] = list(set(dictionary.get(t)))\n",
    "#         else:\n",
    "#             dictionary[t] = [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {}\n",
    "def biwordindexing():\n",
    "    for i in range(1,51):\n",
    "        file_name = (\"refined/\"+str(i)+\".txt\")\n",
    "        text = open(file_name, \"r\").read()\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        for j in range(0,len(tokens)-1):\n",
    "            t = tokens[j]+\" \"+tokens[j+1]\n",
    "            if t in dictionary:\n",
    "                dictionary.get(t).append(i)\n",
    "                dictionary[t] = list(set(dictionary.get(t)))\n",
    "            else:\n",
    "                dictionary[t] = [i]\n",
    "biwordindexing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary = {}\n",
    "# for i in range(1, (DOCSIZE+1)):\n",
    "#     file_name = (\"refined/\"+str(i)+\".txt\")\n",
    "#     text = open(file_name, \"r\").read()\n",
    "#     tokens = tokenizer.tokenize(text)\n",
    "#     for t in tokens:\n",
    "#         if t in dictionary:\n",
    "#             dictionary.get(t).append(i)\n",
    "#             dictionary[t] = list(set(dictionary.get(t)))\n",
    "#         else:\n",
    "#             dictionary[t] = [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dont do everytime\n",
    "# text_file = open(\"inverted_list.csv\", \"w\")\n",
    "# text_file.write(\"Words, Inverted Index \\n\")\n",
    "# for item in dictionary.keys():\n",
    "#     join_items = \", \".join(str(d) for d in dictionary.get(item))\n",
    "#     text = item+\", \"+str(join_items)\n",
    "#     text_file.write(text+\" \\n\")\n",
    "# text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_intersect(list1, list2):\n",
    "    mer_list = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while (i<len(list1) and j<len(list2)):\n",
    "        if (list1[i] == list2[j]):\n",
    "            mer_list.append(list1[i])\n",
    "            i = i+1\n",
    "            j = j+1\n",
    "        else:\n",
    "            if (list1[i] > list2[j]):\n",
    "                j = j+1\n",
    "            else:\n",
    "                i = i+1\n",
    "    return mer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def or_intersect(list1, list2):\n",
    "    mer_list = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while (i<len(list1) and j<len(list2)):\n",
    "        if (list1[i] == list2[j]):\n",
    "            mer_list.append(list1[i])\n",
    "            i = i+1\n",
    "            j = j+1\n",
    "        else:\n",
    "            if (list1[i] > list2[j]):\n",
    "                mer_list.append(list2[j])\n",
    "                j = j+1\n",
    "            else:\n",
    "                mer_list.append(list1[i])\n",
    "                i = i+1\n",
    "    return mer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_list(list1):\n",
    "    not_list = []\n",
    "    for i in range(1, (DOCS_SIZE + 1)):\n",
    "        if i in list1:\n",
    "            pass\n",
    "        else:\n",
    "            not_list.append(i)\n",
    "    return not_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_binary_operations(word1, word2):\n",
    "    list_1 = dictionary.get(word1)\n",
    "    list_2 = dictionary.get(word2)\n",
    "    lists_and = and_intersect(list_1, list_2)\n",
    "    lists_or = or_intersect(list_1, list_2)\n",
    "    list1_not = not_list(list_1)\n",
    "    list2_not = not_list(list_2)\n",
    "    text_file = open(word1+\"_\"+word2+\"_operation.csv\", \"w\")\n",
    "    text_file.write(\"Words, Inverted Index \\n\")\n",
    "    join_items = \", \".join(str(d) for d in list_1)\n",
    "    text = word1+\", \"+str(join_items)\n",
    "    text_file.write(text+\" \\n\")\n",
    "    join_items = \", \".join(str(d) for d in list_2)\n",
    "    text = word2+\", \"+str(join_items)\n",
    "    text_file.write(text+\" \\n\")\n",
    "    join_items = \", \".join(str(d) for d in lists_and)\n",
    "    text = word1+\" AND \"+word2+\", \"+str(join_items)\n",
    "    text_file.write(text+\" \\n\")\n",
    "    join_items = \", \".join(str(d) for d in lists_or)\n",
    "    text = word1+\" OR \"+word2+\", \"+str(join_items)\n",
    "    text_file.write(text+\" \\n\")\n",
    "    join_items = \", \".join(str(d) for d in list1_not)\n",
    "    text = \"NOT \"+word1+\", \"+str(join_items)\n",
    "    text_file.write(text+\" \\n\")\n",
    "    join_items = \", \".join(str(d) for d in list2_not)\n",
    "    text = \"NOT \"+word2+\", \"+str(join_items)\n",
    "    text_file.write(text+\" \\n\")\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_binary_operations(\"know\", \"far\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_and(list1):\n",
    "    and_list = and_intersect(list1[0], list1[1])\n",
    "    for i in range(2, len(list1)):\n",
    "        and_list = and_intersect(list1[i], and_list)\n",
    "    return and_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_or(list1):\n",
    "    or_list = or_intersect(list1[0], list1[1])\n",
    "    for i in range(2, len(list1)):\n",
    "        or_list = or_intersect(list1[i], or_list)\n",
    "    return or_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_list = [dictionary.get('come'), dictionary.get('leave'), dictionary.get('know'), dictionary.get('far')]\n",
    "multiple_and(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code to make inverted index - not working\n",
    "# import InvertedIndex\n",
    "# import InvertedIndexQuery\n",
    "\n",
    "# i = InvertedIndex.Index()\n",
    "\n",
    "# filename = '1.txt'\n",
    "# file_to_index = open(filename).read() \n",
    "# document_key = filename\n",
    "\n",
    "#     # index the document, using document_key as the document's\n",
    "#     # id.\n",
    "# i.index(file_to_index, document_key)\n",
    "\n",
    "# filename = '2.txt'\n",
    "# file_to_index = open(filename).read()\n",
    "# document_key = filename\n",
    "\n",
    "# i.index(file_to_index, document_key)\n",
    "\n",
    "# search_results = InvertedIndexQuery.query('Python and spam', i)\n",
    "# search_results.sort()\n",
    "\n",
    "# cnt = 0\n",
    "# for document in search_results:\n",
    "#     cnt = cnt + 1\n",
    "#     print ('%d) %s'.format(cnt, document[1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.get(\"vanish adobe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
